{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "remarkable-carrier",
   "metadata": {},
   "source": [
    "# File I/O and NumPy\n",
    "\n",
    "Now we have the ability to perform NumPy array computation and manipulation and we know how to construct a record array, it's time for us to do some real-world analysis by reading files into a NumPy array and outputing the result array to a file for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-embassy",
   "metadata": {},
   "source": [
    "In this section, you will learn how to load/import your data and save it. There are many different ways of loading data, and the right way depends on your file `type`. You can load/import text files, SAS/Stata files, HDF5 files, and many others. HDF (Hierarchical Data Format) is one of the popular data formats which is used to store and organize large amounts of data and it is very useful while working with a multidimensional homogeneous arrays. For example, Pandas library has a very handy class named as HDFStore where you can easily work with HDF5 files. While working on data science projects, you will most likely see many of these types of files, but in this book, we will cover the most popular ones, such as **NumPy binary files**, **text files** (`.txt`), and **Comma Separated Values** (`.csv`) files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-hours",
   "metadata": {},
   "source": [
    "## Text and CSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-panel",
   "metadata": {},
   "source": [
    "We should talk about reading the file first and then exporting the file. But now, we are going to reverse the process, and create a record array first and then output the array to a CSV file. We read the exported CSV file into the NumPy record arrays and compared it with our original record array. The sample array we're going to create will contain an `id` field with consecutive integers, a `value` field containing random floats, and a `date` field with `numpy.datetime64['D']`. This exercise will use all the knowledge you gained from the previous sections and chapters. Let's start creating the record array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "express-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "focused-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = np.arange(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "urban-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = np.random.random(1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "blind-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = np.random.randint(0, 365, 1000) * np.timedelta64(1, 'D') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "historic-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = np.datetime64('2014-01-01') + day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "administrative-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_array = np.core.records.fromarrays(\n",
    "    [id_, value, date],\n",
    "    names='id, value, date',\n",
    "    formats='i4, f4, a10'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "urban-black",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.array([(0, 0.94466794, b'2014-03-21'), (1, 0.990071  , b'2014-06-12'),\n",
       "           (2, 0.39904514, b'2014-11-22'), (3, 0.92618376, b'2014-10-26'),\n",
       "           (4, 0.15883118, b'2014-01-12')],\n",
       "          dtype=[('id', '<i4'), ('value', '<f4'), ('date', 'S10')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_array[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-observer",
   "metadata": {},
   "source": [
    "We first create three NumPy arrays representing the fields we need: `id`, `value`, and `date`. When creating the `date` field, we combine the `numpy.datetime64` with a random NumPy array with size 1000 to simulate random dates in the range from `2014-01-01` to `2014-12-31` (365 days)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-excuse",
   "metadata": {},
   "source": [
    "Then we use the `numpy.core.records.fromarrays()` function to merge the three arrays into one record array and assign the `names` (field name) and the `formats` (data type). One thing to notice here is that the record array doesn't support the `numpy.datetime64` object, so we stored it in the array as a date/time string with a length of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-month",
   "metadata": {},
   "source": [
    "If you are using Python 3, you will find a prefix `b` added to the front of the date/time string in the record array such as `b'2014-09-25'`. `b` here stands for \"bytes literals\" meaning it only contains ASCII characters (all string types in Python 3 are Unicode, which is one major change between Python 2 and 3). Therefore in Python 3, converting an object (datetime64) to a string will add the prefix to differentiate between the normal string type. However, it doesn't affect what we are going to do next-exporting the record array to a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "exterior-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./record.csv', rec_array, fmt='%i, %.4f, %s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-exclusive",
   "metadata": {},
   "source": [
    "We use the `numpy.savetxt()` function to handle the exporting, and we specify the first argument as the exported file location, the array name, and the format using the `fmt` argument. We have three fields with three different data types and we want to add `,` in between each field in the CSV file. If you prefer any other delimiters, replace the comma in the `fmt` argument. We also get rid of redundant digits in the value field, so we specify only four digits after the decimal points to the file by using `%.4f`. Now you may go to the file location we specified in the first argument to check the CSV file. Open it in a spreadsheet software program and you can see the following:\n",
    "\n",
    "```csv\n",
    "0, 0.5038, b'2014-04-22'\n",
    "1, 0.6160, b'2014-06-05'\n",
    "2, 0.7191, b'2014-08-05'\n",
    "3, 0.3750, b'2014-12-14'\n",
    "4, 0.9078, b'2014-07-01'\n",
    "5, 0.9822, b'2014-03-01'\n",
    "6, 0.5804, b'2014-01-03'\n",
    "7, 0.7590, b'2014-04-02'\n",
    "8, 0.8939, b'2014-08-28'\n",
    "9, 0.1700, b'2014-07-30'\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-housing",
   "metadata": {},
   "source": [
    "Next, we are going to read the CSV file to a record array and use the `value` field to generate a mask field, named `mask`, which represents a value larger than or equal to 0.75. Then we will append the new mask field to the record array. Let's read the CSV file first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "external-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_array = np.genfromtxt(\n",
    "    './record.csv',\n",
    "    dtype='i4,f4,a10',\n",
    "    delimiter=',',\n",
    "    skip_header=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "advance-copyright",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('f0', '<i4'), ('f1', '<f4'), ('f2', 'S10')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_array.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-sheriff",
   "metadata": {},
   "source": [
    "We use `numpy.genfromtxt()` to read the file into NumPy record array. The first argument is still the file location we want to access, and the `dtype` argument is optional. If we didn't specify this, NumPy will determine the `dtype` argument using the contents of each column individually. Since we clearly know about the data, it's recommended to specify every time when you read the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-prison",
   "metadata": {},
   "source": [
    "The `delimiter` argument is also optional, and by default, any consecutive whitespaces act as delimiters. However, we used `\",\"` for the CSV file. The last optional argument we use in the method is the `skip_header`. Although we didn't have the field name on top of the records in the file, NumPy provides the functionality to skip a number of lines at the beginning of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-consciousness",
   "metadata": {},
   "source": [
    "Other than `skip_header`, the `numpy.genfromtext()` function supports 22 more operation parameters to fine-tune the array, such as defining missing and filling values. For more details, please refer to https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.genfromtxt.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-fundamental",
   "metadata": {},
   "source": [
    "Now the data is read in to the record array, you will find that the second field is more than four digits after the decimal points as we specified in exporting the CSV. The reason for this is because we use `f4` as its data type when read in. The empty digits will be filled by NumPy, but the valid four digits remain the same as in the file. You may also notice we lost the field name, so let's specify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "personalized-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_array.dtype.names = ('id', 'value', 'date') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-prerequisite",
   "metadata": {},
   "source": [
    "## `.npy` or `.npz`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-fight",
   "metadata": {},
   "source": [
    "When you are working with arrays, you will usually save them as NumPy binary files after you have finished working with them. The reason for this is that you need to store your array shape and data type as well. When you reload your array, you expect NumPy to remember it, and you can continue working from where you left off. Moreover, NumPy binary files can store information about an array, even when you open the file on another machine with a different architecture. In NumPy, the `load()`, `save()`, `savez()`, and `savez_compressed()` methods help you to load and save NumPy binary files as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "attached-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_array = np.arange(12).reshape(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sorted-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for why `allow_pickle` is set to false, read the note mentioned in a few lines below.\n",
    "np.save('example.npy', example_array, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "green-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load('example.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "twelve-addiction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-pittsburgh",
   "metadata": {},
   "source": [
    "In the preceding code, we execute the following steps to a practice saving array as a binary file and how to load it back without affecting its shape:\n",
    "\n",
    "- Create an array with a shape `(3, 4)`\n",
    "- Save the array as a binary file\n",
    "- Load back the array\n",
    "- Check whether the shape is still the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-fitting",
   "metadata": {},
   "source": [
    "> **Note:** Set `allow_pickle=False` in `numpy.save` and `numpy.load` unless the array `dtype` includes Python objects, in which case pickling is required. Pickles are not secure against erroneous or maliciously constructed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-permission",
   "metadata": {},
   "source": [
    "Similarly, you can use the `savez()` function to save several arrays into a single file. If you want to save your files as compressed NumPy binary files, you can use `savez_compressed()` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecological-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(5)\n",
    "y = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cheap-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('x_y.npz', x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "diverse-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "npzfile = np.load('x_y.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "satellite-lloyd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arr_0', 'arr_1']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npzfile.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bottom-thermal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npzfile['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "confused-emission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npzfile['arr_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-unknown",
   "metadata": {},
   "source": [
    "When you save several arrays in a single file, if you give a keyword argument such as `first_array=x`, your array will be saved with this name. Otherwise, by default, your first array will be given a variable name, such as `arr_0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "textile-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('x_y_compressed.npz', first_array=x , second_array=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "complete-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "npzfile = np.load('x_y_compressed.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "endangered-blues",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first_array', 'second_array']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npzfile.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "auburn-constant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npzfile['first_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "increasing-throw",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npzfile['second_array']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-biography",
   "metadata": {},
   "source": [
    "> **Note:** In general, prefer `numpy.save` and `numpy.load` to `numpy.ndarray.tofile` and `numpy.fromfile` as they lose information on endianness and precision and so are unsuitable for anything but scratch storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-salad",
   "metadata": {},
   "source": [
    "## `json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-result",
   "metadata": {},
   "source": [
    "> **Warning:** NumPy arrays are not directly JSON serializable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
